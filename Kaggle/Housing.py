"""Housing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yzuvm1Aw-zxm4BC_5HX7UMfv1tMeSWM6
"""

import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
import tensorflow as tf
import keras
from sklearn.preprocessing import LabelEncoder 
import seaborn as sns
import scipy.stats as s
from scipy.stats import pearsonr as p
from statsmodels.stats.stattools import durbin_watson as dw
from scipy.special import boxcox1p as b
from scipy.stats import boxcox_normmax as n
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from sklearn.impute import SimpleImputer  
from sklearn.model_selection import GridSearchCV,KFold,cross_val_score
from sklearn.metrics import mean_squared_log_error

df= pd.read_csv('train.csv')
df1= pd.read_csv('test.csv')

"""Removing features with >40% len(df) null values"""

varwithhighnullvalues=[]
for i in df.columns:
  if df[i].isnull().sum(0)>(0.4*len(df)):
    varwithhighnullvalues.append(i)
    df=df.drop(i,1)
    df1=df1.drop(i,1)
varwithhighnullvalues

catvarwithnullvalues={}
for i in df.columns:
  if i not in df._get_numeric_data().columns and df[i].isnull().sum(0)!=0:
    catvarwithnullvalues[i]=df[i].isnull().sum(0),df[i].nunique()
len(catvarwithnullvalues),catvarwithnullvalues

from scipy.stats import f_oneway as anova
lb=LabelEncoder()
a=lb.fit_transform(df['BsmtFinType1'])
b=lb.fit_transform(df['BsmtFinType2'])
anova(a,b)

lb.fit_transform(df['BsmtFinType1'].values)

g=pd.DataFrame(df[catvarwithnullvalues.keys()])
g.describe()

for i in catvarwithnullvalues:
  df[i[0]]=df[i[0]].fillna(method='ffill')

categorical_variables=[i for i in df.columns if i not in df._get_numeric_data().columns]
len(categorical_variables)

plt.figure(figsize=(10,10))
for i,j in enumerate(['BsmtFinType1','BsmtFinType2']):
  plt.subplot(1,2,i+1)
  sns.swarmplot(df[j],df['SalePrice'])

lb=LabelEncoder()
for i in categorical_variables:
  df[i]=lb.fit_transform(df[i].values)

df=df.drop('GarageCond',1)
df1=df1.drop('GarageCond',1)

for i in categorical_variables:
  dummy_var=pd.get_dummies(df[i],drop_first='True')
  df=df.join(dummy_var,lsuffix=str(str(i)+'l'))
  df1=df1.join(dummy_var,lsuffix=str(str(i)+'l'))
  df=df.drop(i,1)
  df1=df1.drop(i,1)
df.head()

from scipy.stats import pearsonr as p
col=df.columns
for i in col:
  if df[i].isnull().sum()>0:
    df[i]=df[i].fillna(method='ffill')
    df1[i]=df1[i].fillna(method='ffill')
for i in col:
  if abs(p(df[i],df['SalePrice'])[0])<0.4:
    df=df.drop(i,1)
    df1=df1.drop(i,1)

df.head(5)

poscorr,negcorr={},{}
for i in df.columns:
  if dw(df[i])<0.1 and len(df[i].value_counts())>2:
    poscorr[i]=dw(df[i])
  elif dw(df[i])>2:
    negcorr[i]=dw(df[i])
poscorrs=sorted(poscorr.items(),key=lambda x:x[1],reverse=True)
negcorrs=sorted(negcorr.items(),key=lambda x:x[1],reverse=True)
poscorrs,negcorrs

for i in list(poscorr.keys()):
  df[i]=[(abs(j-df[i].mean())) for j in df[i].values]

m=pd.Series(df['YearRemodAdd']-df['YearBuilt'],name='btnyear')
df=df.join(m)
df=df.drop(['YearRemodAdd','YearBuilt'],1)

m=pd.Series(df1['YearRemodAdd']-df1['YearBuilt'],name='btnyear')
df1=df1.join(m)
df1=df1.drop(['YearRemodAdd','YearBuilt'],1)

df=df.drop('Id',1)
df1=df1.drop('Id',1)

m=pd.Series(df['YrSold']-(df['YrSold'].min()-1),name='yearsold')
df=df.join(m)
df1=df1.join(m)
df=df.drop(['YrSold'],1)
df1=df1.drop(['YrSold'],1)

poskew,negskew={},{}
for i in df.columns:
  if df[i].skew()<=-3:
    negskew[i]=df[i].skew()
  elif df[i].skew()>=1.5:
    poskew[i]=df[i].skew()
poskew_list=sorted(poskew.items(),key=lambda x:x[1],reverse=False)
negskew_list=sorted(negskew.items(),key=lambda x:x[1],reverse=False)

poskew_list

for i in poskew.keys():
  try:df[i]=b(df[i],n(df[i])+1)
  except ValueError:
    continue

for i in negskew.keys():
  try:df[i]=b(df[i],n(df[i])+1)
  except ValueError:
    continue

df.head(5)

"""Checking unique data in columns, checking if some have <2."""

similar=[(i,df[i].nunique(0)) for i in df.columns if df[i].nunique(0)<2]
similar1=[(i,df1[i].nunique(0)) for i in df1.columns if df1[i].nunique(0)<2]
similar,similar1

c=0
l=[]
for i in df.columns:
  if len(df[i].unique())==1:
    l.append(i)
    c+=1
df=df.drop(l,1)

c=0
l=[]
for i in df1.columns:
  if len(df1[i].unique())==1:
    l.append(i)
    c+=1
df1=df1.drop(l,1)

df2=pd.DataFrame()
for i in df.columns:
  df2[i]=df[i].rolling(3).mean()
  m=np.array(df2[i].fillna(method='bfill'))
  sigma=2*(df[i].std())
  for a,b in zip(df[i],m):
    if b-(3*sigma)>=a<=b+(3*sigma):
      df[i]=df[i].replace(a,b)

df['FullBath'].plot()

df=df.drop(['1stFlrSF','GarageArea'],1)
df1=df1.drop(['1stFlrSF','GarageArea'],1)


"""there are few inf values is dataframe hence removed

taking the SalePrice to the end of DataFrame also name changed..
"""

s=pd.Series(df['SalePrice'],name='Saleprice')
df=df.join(s)
df=df.drop('SalePrice',1)

model=tf.keras.models.Sequential([tf.keras.layers.Dense(9,activation='relu',input_shape=(16,)),
                                  tf.keras.layers.Dense(5,activation='relu'), 
                                  tf.keras.layers.Dense(3,activation='softmax')])
model.compile(tf.keras.optimizers.Adam(learning_rate=0.01),tf.keras.losses.MeanSquaredError(),metrics=['accuracy'])

model.summary()

X=df.iloc[:,0:18].values
y=df.iloc[:,18].values
X1=df.iloc[:,0:18].values
sc=StandardScaler()
im=SimpleImputer(missing_values=np.nan, strategy='mean')
#X=im.fit_transform(X)
X1=im.fit_transform(X1)
X_trans=sc.fit_transform(X)
X_trans1=sc.fit_transform(X1)
y_trans=np.asarray([((i-y.min())/(y.max()-y.min())*(2)) for i in y])
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,shuffle=False)

model.fit(X_train,y_train,epochs=100,batch_size=10)

from sklearn.metrics import r2_score 
y_pred=model.predict(X_test)
plt.figure(figsize=(15,10))
plt.plot(y_pred,c='blue')
plt.plot(y_test,c='red')
r2_score(y_test,y_pred)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score 
lr=LinearRegression()
lr.fit(X_train,y_train)
y_pred=lr.predict(X_test)
plt.figure(figsize=(15,10))
plt.plot(y_pred,c='blue')
plt.plot(y_test,c='red')
r2_score(y_test,y_pred)

import statsmodels.api as sm
Model=sm.OLS(y,X)
r=Model.fit()
r.summary()

from sklearn.ensemble import RandomForestRegressor

rfclf=RandomForestRegressor(random_state=0)
params={'n_estimators':[10,15,20,25,30,35,40,45,50],
        'criterion':['mse','mae']}

score=GridSearchCV(rfclf,param_grid=params,scoring='r2',cv=split)
score.fit(X_train,y_train)
score.best_params_

rfclf=RandomForestRegressor(criterion='mse',n_estimators=50).fit(X_train,y_train)
ypredrf=rfclf.predict(X_test)
r2_score(y_test,ypredrf)

ypredrftest=rfclf.predict(X_trans1)

l=open("/content/drive/My Drive/stats/sdc.txt",'w')
for i in ypred:
  l.write(str(i))
  l.write('\n')
l.close()

for i in ypredrftest:
  print((i*(max(y)-min(y)))+min(y))

from google.colab import drive
drive.mount('/content/drive')

from sklearn.ensemble import BaggingRegressor
bagclf=BaggingRegressor(rfclf,random_state=0)
param={'n_estimators':[20,30,40,50,70,80,90,100]}
score=GridSearchCV(rfclf,param_grid=param,scoring='r2',cv=split)
score.fit(X_train,y_train)
score.best_params_

bagclf=BaggingRegressor(rfclf,40,random_state=0).fit(X_train,y_train)
ypredrfbag=rfclf.predict(X_test)
r2_score(y_test,ypredrfbag)

from sklearn.ensemble import GradientBoostingRegressor
gbrclf=GradientBoostingRegressor(random_state=0)
param={'loss':['ls','lad','huber','quantile'],
       'learning_rate':[0.01,0.05,0.5,1],
       'n_estimators':[50,70,80],
       'criterion':['friedman_mse', 'mse', 'mae']}
score=GridSearchCV(gbrclf,param_grid=param,scoring='r2',cv=split)
score.fit(X_train,y_train)
score.best_params_

gbrclf=GradientBoostingRegressor(loss='lad',learning_rate=0.05,n_estimators=80,criterion='friedman_mse',random_state=0).fit(X_train,y_train)
ypredrfgbr=gbrclf.predict(X_test)
r2_score(y_test,ypredrfgbr)

from xgboost import XGBRegressor
xgboost=XGBRegressor(random_state=0)
split=KFold(10)
param={'max depth':[3,5,8,10],
       'learning_rate':[0.01,0.05,0.5,1],
       'n_estimators':[50,70,80],
       'eval_metric':['rmse','r2','mae','auc']}
score=GridSearchCV(xgboost,param_grid=param,scoring='r2',cv=split)
score.fit(X_train,y_train)
score.best_params_

xgboost=XGBRegressor(learning_rate= 0.05,max_depth=3,n_estimators=70,random_state=0,objective='reg:squarederror',eval_metric='rmse').fit(X_train,y_train)
ypredxgboost=xgboost.predict(X_test)
math.sqrt(mean_squared_log_error(y_test,ypredxgboost))

